<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Data Scientist by day, Data Superhero by night.
		     </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					<div id="intro"> 
						<h1>PORTFOLIO FOR<br />
						OLISEDEME CHINONYE FAVOUR</h1>
						<p>Hi there! Welcome to my space. I'm a data Scientist and I really love my job. My skill set encompasses data management, analysis,machine learning and visualization- all designed to provide your business with valuable, data-driven outcomes.
							<p>I develop predictive models that give your business a glimpse into the future. Each tool is chosen specifically to best solve the task at hand.
								</p><a href="www.linkedin.com/in/favour-c-olisedeme-542357108/"> Say 'Hi' on LinkedIn  </a>.</p>
						<ul class="actions">
							<li><a href="#header" class="button icon solid solo fa-arrow-down scrolly">Continue</a></li>
						</ul>
					</div>

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Data Projects</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html">Data Projects</a></li>
							<!-- <li><a href="generic.html">Generic Page</a></li>
							<li><a href="elements.html">Elements Reference</a></li> -->
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/favour-c-olisedeme-542357108/" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="https://www.linkedin.com/in/favour-c-olisedeme-542357108/" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="https://www.linkedin.com/in/favour-c-olisedeme-542357108/" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
							<li><a href="https://github.com/NonyeJ" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Tools Arsenal -->
							<article class="post featured">
								<header class="major">
									<h2><a href="#">My Tool 'Box'</a></h2>
									<p>Tools are exciting and are constantly evolving. I love working with tools to make my work more efficient, but understandng the business problem plus the right tool is the combo I'm excited about.
										Nonetheless, my tool 'box':
									<li>Programming Language: Python, SQL & R.
									<li>Data Analysis & Visualization: SQL, Power BI, Tableau,Matplotlib
									<li>Power Platform Development: Power Apps, Power Automate, & SharePoint.
									<li>Deploy: Azure.
									<p><p></p>Technolgy evolves and so do we.</p>
									<a href="https://drive.google.com/drive/folders/1vchtDEFhhYsdOH59dX7FXPCi-4EdFzCq?usp=sharing" class="button large">See my resume here <a href="https://www.linkedin.com/in/olisedeme-chinonye-favour-542357108"></a></li>
								</ul>
							</article>
						<!-- Featured Post -->
						<article class="post featured">
							<header class="major">
								<h2><a href="https://github.com/NonyeJ/Predicting-Income-with-Python-Classification-Algorithms">Predicting Income with Python Classification Algorithms</h2></a></a>
								    <p>This task focused on using classification algorithms to predict income levels in a selected demographic, utilizing Python's robustness and Azure MLD's cloud-based platform. The analysis was performed on the Adult Dataset from the UCI Machine Learning Repository.
									    <p>The goal was to gain insights into income level determinants and broader socio-economic dynamics.<p></p>
										Three classification algorithms (Random Forest, KNN, Decision Tree) were applied, with Random Forest showing a good performance score of approximately 85.7% accuracy and a high ROC-AUC score, indicating effective class separation and robustness against overfitting.
										<p></p>The EDA and recommendations for this analysis, as well as a comprehensive report,are detailed in the github repository for this project.<p></p>
										<p> This project has also been done using Azure Machine Learning Designer. The details are in the report.
							</header>
							<a href="https://github.com/NonyeJ/Predicting-Income-with-Python-Classification-Algorithms" class="image main"><img src="images/classfifcation.JPG" alt="" /></a>
							<ul class="actions special">
								<li><a href="https://github.com/NonyeJ/Predicting-Income-with-Python-Classification-Algorithms" class="button large">View Project</a></li>
							</ul>
						</article>	
						<!-- Featured Post -->
						<!-- Featured Post -->
						<article class="post featured">
							<header class="major">
								<h2><a href="https://github.com/NonyeJ/Web-Traffic-Time-Analysis">Web Traffic Time Series Forecasting</h2></a></a>
								    <p>This project focuses on forecasting web traffic for various Wikipedia articles, using a collection of sophisticated time series analysis models implemented in Python.
									    <p>The objective was to accurately predict daily views, facilitating better resource allocation and strategic planning for Wikipedia. Insights derived from this analysis could also inform broader content strategy adaptations.<p></p>
										Four different time series forecasting methods were employed—ARIMA, LSTM, XGBoost, and Prophet—with each model tailored to capture specific aspects such as seasonality, trend, and random fluctuations inherent in web traffic data.
										<p></p>LSTM and Prophet models, in particular, demonstrated robust performance, capturing complex patterns effectively.<p></p>
										<p>The analysis, detailed explorations, model comparisons, and comprehensive evaluations are thoroughly documented in the GitHub repository. This project also includes visualizations of predictions versus actual data, offering a clear view of model accuracy and performance.
							</header>
							<a href="https://github.com/NonyeJ/Web-Traffic-Time-Analysis" class="image main"><img src="images/webbbtraffic.jpg" alt="" /></a>
							<ul class="actions special">
								<li><a href="https://github.com/NonyeJ/Web-Traffic-Time-Analysis" class="button large">View Project</a></li>
							</ul>
						</article>	
						<!-- Featured Post -->
						<!-- Featured Post -->
						<article class="post featured">
							<header class="major">
								<h2><a href="https://github.com/NonyeJ/A-Clustering-Study-Customer-Segmentation-and-Behavior">A Clustering Study: Customer Segmentation and Behavior 
									Analysis</h2></a></a>
								    <p>The goal of this task was to apply clustering algorithms to the UCI Online Retail Store Dataset, focusing on customer segmentation and behavior analysis.<p>
										Two clustering algorithms, K-Means and Hierarchical Clustering, were applied. The optimal number of clusters was determined using the WCSS Elbow Method and Silhouette Scoring Method. 
										The analysis included examining the distribution of the 'Quantity' feature across clusters through boxplots and visualizing clusters in 2D scatter plots. Clusters were characterized by various purchasing behaviors: mainstream transactions, higher value transactions, bulk purchases, and outlier transactions.
										<p></p>The EDA and recommendations for this analysis, as well as a comprehensive report,are detailed in the github repository for this project.<p></p>

							</header>
							<a href="https://github.com/NonyeJ/A-Clustering-Study-Customer-Segmentation-and-Behavior" class="image main"><img src="images/clustering.JPG" alt="" /></a>
							<ul class="actions special">
								<li><a href="https://github.com/NonyeJ/A-Clustering-Study-Customer-Segmentation-and-Behavior" class="button large">View Project</a></li>
							</ul>
						</article>	
						<!-- Featured Post -->
						<!-- Featured Post -->
						<article class="post featured">
							<header class="major">
								<h2><a href="https://github.com/NonyeJ/Streaming-sentiments-and-text-mining">Analyzing Netflix’s Trends: Streaming sentiments and text mining</h2> </a></a>
								    <p>This project focuses on analyzing text data from the Netflix Movie and TV Show dataset to understand the sentiments and narratives in the streaming content industry. Utilizing Python libraries and NLTK for sentiment intensity analysis, the project aims to quantify emotional content and uncover trends in various dimensions such as genre, providing a data-driven narrative of the industry's landscape.<p>
										<p><p>The analysis involved sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), frequency analysis, topic modeling, word clouds, and N-gram analysis. Sentiment scores were computed and analyzed through histograms, revealing a distribution with a peak around the neutral zone. Genre sentiment analysis was conducted by splitting and expanding genres, and average sentiment scores for top actors were plotted.</p>
										<p></p>The EDA and recommendations for this analysis, as well as a comprehensive report,are detailed in the github repository for this project.<p></p>
							</header>
							<a href="https://github.com/NonyeJ/Streaming-sentiments-and-text-mining" class="image main"><img src="images/cover-netflix-stock-laptop-trading-logo.jpg" alt="" /></a>
							<ul class="actions special">
								<li><a href="https://github.com/NonyeJ/Streaming-sentiments-and-text-mining" class="button large">View Project</a></li>
							</ul>
						</article>	
						<!-- Featured Post -->	
						<!-- Featured Post -->
						<article class="post featured">
							<header class="major">
								<h2><a href="https://github.com/NonyeJ/Global-Health-Dynamics-Demographics-and-Fiscal-Impacts">Global Health Dynamics: Demographics and Fiscal Impacts using R</h2></a></a>
								    <p>This research provides a detailed analysis of global health systems, examining the interplay between demographics, fiscal allocation, mortality, health system equity, and efficacy from 2010 to 2021. It employs a blend of qualitative, quantitative, and domain-specific methodologies to offer a holistic view of health systems across various socioeconomic contexts. The primary aim is to identify patterns, disparities, and correlations, offering insights into global health systems to inform policies, best practices, and data scientists focusing on economics.<p>
										<p></p>The analysis encompassed descriptive statistics to understand data distribution and properties. Statistical measures such as mean, median, mode, standard deviation, skewness, kurtosis, range, IQR, variance, and quantiles were calculated. Visual EDA included boxplots, histograms, scatterplots, and multivariate plots to understand data distribution, outliers, and relationships among variables.<p></p>
									</p>The EDA and recommendations for this analysis, as well as a comprehensive report,are detailed in the github repository for this project.</p>
									</header>
							<a href="https://github.com/NonyeJ/Global-Health-Dynamics-Demographics-and-Fiscal-Impacts" class="image main"><img src="images/stats.JPG" alt="" /></a>
							<ul class="actions special">
								<li><a href="https://github.com/NonyeJ/Global-Health-Dynamics-Demographics-and-Fiscal-Impacts" class="button large">View Project</a></li>
							</ul>
						</article>
						<!-- Featured Post -->
						<article class="post featured">
							<header class="major">
								<h2><a href="https://github.com/NonyeJ/Google-Stocks-Time-Series-Forecasting-with-SARIMA-Model">TIME SERIES FORECASTING WITH SARIMA MODEL</h2></a></a>
								    <p>ARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. This is one of the easiest and effective machine learning algorithm to performing time series forecasting. This is the combination of Auto Regression and Moving average. ARIMA is an algorithm used for forecasting Time Series Data. Using this model, we can analyze and model time-series data to make future decisions. Some of the applications of Time Series Forecasting are weather forecasting, sales forecasting, business forecasting, stock price forecasting,etc For stationary data, ARIMA model is required; seasonal data, Seasonal ARIMA (SARIMA) is required.<p>
							</header>
							<a href="https://github.com/NonyeJ/Google-Stocks-Time-Series-Forecasting-with-SARIMA-Model" class="image main"><img src="images/Time Series Forecasting with SARIMA Model.jpg" alt="" /></a>
							<ul class="actions special">
								<li><a href="https://github.com/NonyeJ/Google-Stocks-Time-Series-Forecasting-with-SARIMA-Modeln" class="button large">View Project</a></li>
							</ul>
						</article>	
						<!-- Featured Post -->	
						<!-- Featured Post -->
						<article class="post featured">
							<header class="major">
								<h2><a href="https://github.com/NonyeJ/D-S-Sales-Dashboard-using-Google-Data-Studio"> Google Data Studio Visualization: D&S Health Sales Dashboard</h2></a></a>
								    <p>The Google ecosystem is a fantastic innovation. It was my first go-to in my journey before I was swayed by Microsoft. For this simple analysis & Visualization, we highlight D&S Health Tech. D&S is a mental health tech-ed organization. D&S has just completed a sales campaign. The Sales team needs some insights(revenue,products,coupons,etc.) from some sales data submitted. This analysis and visulaization was done using Google Data Studio, while the dataset was collected on Google Sheets.<p>
							</header>
							<a href="https://github.com/NonyeJ/D-S-Sales-Dashboard-using-Google-Data-Studio" class="image main"><img src="images/data studio.jpg" alt="" /></a>
							<ul class="actions special">
								<li><a href="https://github.com/NonyeJ/D-S-Sales-Dashboard-using-Google-Data-Studio" class="button large">View Project</a></li>
								<li><a href="https://lookerstudio.google.com/reporting/a6c34151-7341-408f-8752-b8ef96b022bb/page/EOU4C" class="button large">View Interactive Dashboard</a></li>
							</ul>
						</article>	
						<!-- Featured Post -->
						<!-- Featured Post -->
						<article class="post featured">
							<header class="major">
								<h2><a href="https://github.com/NonyeJ/Customer-Churn-Prediction">Data Exploratory Analysis with Python</h2></a></a>
								    <p>Our client is a major gas and electricity utility that supplies to corporate, SME (Small & Medium Enterprise), and residential customers. The power-liberalization of the energy market in Europe has led to significant customer churn, especially in the SME segment. They want to diagnose the source of churning SME customers.
									A fair hypothesis is that price changes affect customer churn. Therefore, it is helpful to know which customers are more (or less) likely to churn at their current price, for which a good predictive model could be useful.
									Moreover, for those customers that are at risk of churning, a discount might incentivize them to stay with our client. The head of the SME division is considering a 20% discount that is considered large enough to dissuade almost anyone from churning (especially those for whom price is the primary concern).
									An initial team meeting was held to discuss various hypotheses, including churn due to price sensitivity. Deeper exploration is required on the hypothesis that the churn is driven by the customers’ price sensitivities.
									The client plans to use the predictive model on the 1st working day of every month to indicate to which customers the 20% discount should be offered.
									We would need to fomulate the hypothesis as a data science problem and lay out the major steps needed to test this hypothesis. Communicate your thoughts and findings to the client, focusing on the data that we would need from the client and the analytical models we would use to test such a hypothesis.<p>
							</header>
							<a href="https://github.com/NonyeJ/Customer-Churn-Prediction" class="image main"><img src="images/Churn.jpg" alt="" /></a>
							<ul class="actions special">
								<li><a href="https://github.com/NonyeJ/Customer-Churn-Prediction" class="button large">View Project</a></li>
							</ul>
						</article>	
						<!-- Featured Post -->
						<article class="post featured">
							<header class="major">
								<h2><a href="https://github.com/NonyeJ/Data-Cleaning-using-SQL">Data Cleaning with SQL</a></h2>
								<p>Data cleaning is a pertininet part of end to end data analysis process. SQL, in my opinion is a great tool. Apt, concise. The quality of the data we use determines the quality of the results and insights we get. Many professionals believe that we should dedicate more time to preparing and cleaning the data rather than other processes like EDA, ML, and others. Otherwise, we could finish with a bunch of inaccurate output.
									In this project I drive a cleaning data process to prepare data for analysis by modifying incomplete data, removing irrelevant and duplicated rows, splitting addresses, and modifying improperly formatted data.						
									Data cleaning is not about erasing information to simplify the dataset, but rather finding a way to maximize the accuracy of the collected data.
									Let’s go over cleaning techniques with a Housing dataset. It has 56K+ rows. Let’s get started!.<p>
							</header>
							<a href="https://github.com/NonyeJ/Data-Cleaning-using-SQL" class="image main"><img src="images/housing.jpg" alt="" /></a>
							<ul class="actions special">
								<li><a href="https://github.com/NonyeJ/Data-Cleaning-using-SQL" class="button large">View Project</a></li>
							</ul>
						</article>	
						<!-- Posts -->
							<section class="posts">
								<article>
									<header>
									<h2><a href="https://github.com/NonyeJ/Data-Exploration-using-SQL-for-case-study-Covid-19">Data Exploration in Structured Query Language<br />
										</a></h2>
									</header>
									<a href="https://github.com/NonyeJ/Data-Exploration-using-SQL-for-case-study-Covid-19" class="image fit"><img src="images/covid 19.jpg" alt="" /></a>
									<p>Covid-19 was tough. It's the first pandemic I have ever witnessed and my world turned 360. This project is a simple data exploration excercise using Structured Query Language with real data sets from the Covid-19 pandemic. I do intend on working more with this data because the dataset is extensive. On Data exploration and aggregation, these are two significant aspects of data analysis while processing with transactional data stored in SQL Server for statistical inferences. For data exploration using data science languages like SQL, data often needs to be filtered, re-ordered, transformed, aggregated and visualized. There are many possibilities for achieving these functionalities.</p>
									<ul class="actions special">
										<li><a href="https://github.com/NonyeJ/Data-Exploration-using-SQL-for-case-study-Covid-19" class="button">View Project</a></li>
									</ul>
								</article>
								<article>
									<header>
										<section class="posts">
											<article>
												<header>
												<h2><a href="https://github.com/NonyeJ/Web-scraping">Web Scraping with Python (BeautifulSoup Lib)<br />
													</a></h2>
												</header>
												<a href="https://github.com/NonyeJ/Web-scraping" class="image fit"><img src="images/webscarping.jpg" alt="" /></a>
												<p>Web scraping (or data scraping) is a technique used to collect content and data from the internet. This data is usually saved in a local file so that it can be manipulated and analyzed as needed. If you’ve ever copied and pasted content from a website into an Excel spreadsheet, this is essentially what web scraping is, but on a very small scale.
													However, when people refer to ‘web scrapers,’ they’re usually talking about software applications. Web scraping applications (or ‘bots’) are programmed to visit websites, grab the relevant pages and extract useful information. By automating this process, these bots can extract huge amounts of data in a very short time. SR:Careerfoundry</p>
												<ul class="actions special">
													<li><a href="https://github.com/NonyeJ/Web-scraping" class="button">View Project</a></li>
												</ul>
											</article>
											<article>
												<header>
										
										<h2><a href="#">Data Visualization with Tableau & Power BI<br />
										</a></h2>
									</header>
									<a href="#" class="image fit"><img src="images/COMING_1.jpg" alt="" /></a>
									<p>The analyst is currently cooking new visualizations. Check back soon!</p>
									<ul class="actions special">
										<li><a href="#" class="button">Coming Soon!</a></li>
									</ul>
																	<header>
									<h2><a href="#">Correlation in Python</a></h2>
									</header>
									<a href="#" class="image fit"><img src="images/COMING_1.jpg" alt="" /></a>
									<p>Currently writing multiple lines of code using python. Please check in soon</p>
									<ul class="actions special">
										<li><a href="#" class="button">Coming soon!</a></li>
									</ul>
								</article>
								<article>
									
						<!-- Footer -->
							<footer>
								<div class="pagination">
									<!--<a href="#" class="previous">Prev</a>-->
									<a href="#" class="page active">1</a>
									<a href="#" class="page">2</a>
									<a href="#" class="page">3</a>
									<span class="extra">&hellip;</span>
									<a href="#" class="page">8</a>
									<a href="#" class="page">9</a>
									<a href="#" class="page">10</a>
									<a href="#" class="next">Next</a>
								</div>
							</footer>

					</div>

				Footer
					<footer id="footer">
						<section>
							<form method="post" action="#">
								<div class="fields">
									<div class="field">
										<label for="name">Name</label>
										<input type="text" name="name" id="name" />
									</div>
									<div class="field">
										<label for="email">Email</label>
										<input type="text" name="email" id="email" />
									</div>
									<div class="field">
										<label for="message">Message</label>
										<textarea name="message" id="message" rows="3"></textarea>
									</div>
								</div>
								<ul class="actions">
									<li><input type="submit" value="Send Message" /></li>
								</ul>
							</form>
						</section>
						<section class="split contact">
							<section class="alt">
								<h3>Address</h3>
								<p>Manchester, United Kingdom<br />
								</p>
							</section>
								
							<section>
								<h3>Email</h3>
								<p><a href="#">contactnonye@gmail.com</a></p>
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="https://twitter.com/_NonyeOJ?t=XKpP13SXwC8lIZ2opfB1tQ&s=03" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="#" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
									<li><a href="#" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
									<li><a href="https://github.com/NonyeJ" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</section>
						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
